{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Será criado um script para passar os dados de um banco de dados sqlite para um delta table, utilizando spark\n",
    "\n",
    "* Para iniciar o pyspark no console* :  pyspark --packages io.delta:delta-core_2.12:2.1.0 --driver-class-path ~/sqlite-jdbc-3.34.0.jar\n",
    "OU\n",
    "Posso fazer como no script e colocar diretamente no config da SparkSession, assim posso chamar o pyspark ou jupyter notebook sem parâmetros.\n",
    "* Documentação com exemplo de utilização do jdbc drive para sqlite:*https://spark.apache.org/docs/latest/api/python/user_guide/pandas_on_spark/from_to_dbms.html*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    " # import libraries\n",
    "from delta.tables import *\n",
    "from pyspark.sql import SparkSession\n",
    "# from pyspark import SparkConf\n",
    "# from pyspark.sql import *\n",
    "import pyspark\n",
    "from delta import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/spark-3.3.2-bin-hadoop3/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/fovr/.ivy2/cache\n",
      "The jars for the packages stored in: /home/fovr/.ivy2/jars\n",
      "io.delta#delta-core_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-640b21d8-4a21-4968-bec3-4fc863882500;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-core_2.12;2.3.0 in central\n",
      "\tfound io.delta#delta-storage;2.3.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.8 in central\n",
      ":: resolution report :: resolve 167ms :: artifacts dl 7ms\n",
      "\t:: modules in use:\n",
      "\tio.delta#delta-core_2.12;2.3.0 from central in [default]\n",
      "\tio.delta#delta-storage;2.3.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.8 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-640b21d8-4a21-4968-bec3-4fc863882500\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/5ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/08 17:07:49 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Adicionado um outro caminho para o warehouse\n",
    "builder = SparkSession.builder.appName(\"Sqlite para Spark\") \\\n",
    ".config(\"spark.sql.warehouse.dir\", '/home/fovr/PROJETOS/olist-ml-models/src') \\\n",
    ".config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:2.1.0\") \\\n",
    ".config(\"spark.jars\", \"/home/fovr/PROJETOS/olist-ml-models/jdbc/sqlite-jdbc-3.34.0.jar\")  \\\n",
    ".config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    ".config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    ".enableHiveSupport() \n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Configurar e ler tabela sqlite\n",
       "Será passado para um um dataframe cada tabela do banco de dados.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%markdown\n",
    "# Configurar e ler tabela sqlite\n",
    "Será passado para um um dataframe cada tabela do banco de dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'jdbc:sqlite:/home/fovr/PROJETOS/olist-ml-models/db/olist.db'\n",
    "driver = 'org.sqlite.JDBC'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tabela: avaliacao_pedido\n",
    "df_avaliacao = spark.read.format('jdbc') \\\n",
    "        .options(driver=driver, dbtable='avaliacao_pedido', url=url)\\\n",
    "        .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------+--------------------+----------------------+-----------+----------+\n",
      "|         idAvaliacao|            idPedido|vlNota|descTituloComentario|descMensagemComentario|dtAvaliacao|dtResposta|\n",
      "+--------------------+--------------------+------+--------------------+----------------------+-----------+----------+\n",
      "|7bc2406110b926393...|73fc7af87114b3971...|   4.0|                null|                  null| 2018-01-18|2018-01-18|\n",
      "+--------------------+--------------------+------+--------------------+----------------------+-----------+----------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_avaliacao.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tabela: cliente\n",
    "df_cliente = spark.read.format('jdbc') \\\n",
    "        .options(driver=driver, dbtable='cliente', url=url)\\\n",
    "        .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------+----------+------+\n",
      "|           idCliente|      idClienteUnico|codCep|descCidade|descUF|\n",
      "+--------------------+--------------------+------+----------+------+\n",
      "|06b8999e2fba1a1fb...|861eff4711a542e4b...| 14409|    franca|    SP|\n",
      "+--------------------+--------------------+------+----------+------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_cliente.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tabela: geolocalizacao\n",
    "df_geolocalizacao = spark.read.format('jdbc') \\\n",
    "        .options(driver=driver, dbtable='geolocalizacao', url=url)\\\n",
    "        .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+-----------+----------+------+\n",
      "|codCep|nrLatitude|nrLongitude|descCidade|descUF|\n",
      "+------+----------+-----------+----------+------+\n",
      "|  1037|      null|       null| sao paulo|    SP|\n",
      "+------+----------+-----------+----------+------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_geolocalizacao.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tabela: item_pedido\n",
    "df_item_pedido = spark.read.format('jdbc') \\\n",
    "        .options(driver=driver, dbtable='item_pedido', url=url)\\\n",
    "        .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+--------------------+--------------------+-------------+-------+-------+\n",
      "|            idPedido|idPedidoItem|           idProduto|          idVendedor|dtLimiteEnvio|vlPreco|vlFrete|\n",
      "+--------------------+------------+--------------------+--------------------+-------------+-------+-------+\n",
      "|00010242fe8c5a6d1...|           1|4244733e06e7ecb49...|48436dade18ac8b2b...|   2017-09-19|   58.9|  13.29|\n",
      "+--------------------+------------+--------------------+--------------------+-------------+-------+-------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_item_pedido.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tabela: pagamento_pedido\n",
    "df_pagamento_pedido = spark.read.format('jdbc') \\\n",
    "        .options(driver=driver, dbtable='pagamento_pedido', url=url)\\\n",
    "        .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------------+-----------------+----------+-----------+\n",
      "|            idPedido|idPagamentoPedido|descTipoPagamento|nrParcelas|vlPagamento|\n",
      "+--------------------+-----------------+-----------------+----------+-----------+\n",
      "|b81ef226f3fe1789b...|                1|      credit_card|         8|      99.33|\n",
      "+--------------------+-----------------+-----------------+----------+-----------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pagamento_pedido.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tabela: pedido\n",
    "df_pedido = spark.read.format('jdbc') \\\n",
    "        .options(driver=driver, dbtable='pedido', url=url)\\\n",
    "        .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------------+----------+----------+----------+----------+-------------------+\n",
      "|            idPedido|           idCliente|descSituacao|  dtPedido|dtAprovado|   dtEnvio|dtEntregue|dtEstimativaEntrega|\n",
      "+--------------------+--------------------+------------+----------+----------+----------+----------+-------------------+\n",
      "|e481f51cbdc54678b...|9ef432eb625129730...|   delivered|2017-10-02|2017-10-02|2017-10-04|2017-10-10|         2017-10-18|\n",
      "+--------------------+--------------------+------------+----------+----------+----------+----------+-------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pedido.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tabela: produto\n",
    "df_produto = spark.read.format('jdbc') \\\n",
    "        .options(driver=driver, dbtable='produto', url=url)\\\n",
    "        .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+-------------+------------------+-------+------------+---------------+----------+-----------+\n",
      "|           idProduto|descCategoria|nrTamanhoNome|nrTamanhoDescricao|nrFotos|vlPesoGramas|vlComprimentoCm|vlAlturaCm|vlLarguraCm|\n",
      "+--------------------+-------------+-------------+------------------+-------+------------+---------------+----------+-----------+\n",
      "|1e9e8ef04dbcff454...|   perfumaria|         40.0|             287.0|    1.0|       225.0|           16.0|      10.0|       14.0|\n",
      "+--------------------+-------------+-------------+------------------+-------+------------+---------------+----------+-----------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_produto.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tabela: vendedor\n",
    "df_vendedor = spark.read.format('jdbc') \\\n",
    "        .options(driver=driver, dbtable='vendedor', url=url)\\\n",
    "        .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+----------+------+\n",
      "|          idVendedor|descCep|descCidade|descUF|\n",
      "+--------------------+-------+----------+------+\n",
      "|3442f8959a84dea7e...|  13023|  campinas|    SP|\n",
      "+--------------------+-------+----------+------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_vendedor.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Ingestão - Criando delta table\n",
       "### Silver\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%markdown\n",
    "# Ingestão - Criando delta table\n",
    "### Silver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/fovr/PROJETOS/olist-ml-models/delta/silver'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Ler o dataframe e realizar a ingestão em um delta table\n",
    "\n",
    "\n",
    "df_avaliacao.write.mode('overwrite').format('delta').partitionBy('dtAvaliacao').save(f'{path}/avaliacao')\n",
    "df_cliente.write.mode('overwrite').format('delta').partitionBy('descUF').save(f'{path}/cliente')\n",
    "df_geolocalizacao.write.mode('overwrite').format('delta').partitionBy('descUF').save(f'{path}/geo')\n",
    "df_item_pedido.write.mode('overwrite').format('delta').save(f'{path}/item_pedido')\n",
    "df_pagamento_pedido.write.mode('overwrite').format('delta').save(f'{path}/pagamento')\n",
    "df_pedido.write.mode('overwrite').format('delta').partitionBy('dtPedido').save(f'{path}/pedido')\n",
    "df_produto.write.mode('overwrite').format('delta').partitionBy('descCategoria').save(f'{path}/produto')\n",
    "df_vendedor.write.mode('overwrite').format('delta').partitionBy('descUF').save(f'{path}/vendedor')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------+--------------------+----------------------+----------+-----------+\n",
      "|         idAvaliacao|            idPedido|vlNota|descTituloComentario|descMensagemComentario|dtResposta|dtAvaliacao|\n",
      "+--------------------+--------------------+------+--------------------+----------------------+----------+-----------+\n",
      "|5bf1a70143253c588...|ab8c4bf0204081a45...|   5.0|                null|                  null|2017-12-19| 2017-12-19|\n",
      "|f5eb912253261b8bb...|dccf99a9f12d7217c...|   4.0|                null|                  null|2017-12-20| 2017-12-19|\n",
      "|608e890852fc78c38...|2d1f07a3af3df1ddc...|   3.0|                null|  Até agora não rec...|2017-12-21| 2017-12-19|\n",
      "|875e5a568b6d64e86...|99b1cf2d12de43d08...|   5.0|                null|                  null|2017-12-19| 2017-12-19|\n",
      "|5fbdc041e09387736...|6c6803a80161b45ad...|   5.0|                null|  Produto original,...|2017-12-20| 2017-12-19|\n",
      "+--------------------+--------------------+------+--------------------+----------------------+----------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.parquet(f'{path}/avaliacao').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/08 17:08:08 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "23/04/08 17:08:08 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n",
      "23/04/08 17:08:10 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 2.3.0\n",
      "23/04/08 17:08:10 WARN ObjectStore: setMetaStoreSchemaVersion called but recording version is disabled: version = 2.3.0, comment = Set by MetaStore fovr@127.0.1.1\n",
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|  default|\n",
      "|   silver|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\" SHOW DATABASES \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/08 16:37:56 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException\n",
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\" USE silver \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/08 00:02:38 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException\n",
      "23/04/08 00:02:38 WARN ObjectStore: Failed to get database silver, returning NoSuchObjectException\n"
     ]
    }
   ],
   "source": [
    "# spark.sql(\"\"\" DROP DATABASE silver\"\"\").show()\n",
    "# spark.sql(\"\"\" CREATE DATABASE silver\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n",
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n",
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n",
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n",
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n",
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n",
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n",
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(f\"\"\" CREATE TABLE IF NOT EXISTS silver.tbAvaliacao USING DELTA LOCATION  \"{path}/avaliacao\" \"\"\").show()\n",
    "spark.sql(f\"\"\" CREATE TABLE IF NOT EXISTS silver.tbCliente USING DELTA LOCATION  \"{path}/cliente\" \"\"\").show()\n",
    "spark.sql(f\"\"\" CREATE TABLE IF NOT EXISTS silver.tbGeolocalizacao USING DELTA LOCATION  \"{path}/geo\" \"\"\").show()\n",
    "spark.sql(f\"\"\" CREATE TABLE IF NOT EXISTS silver.tbItemPedido USING DELTA LOCATION  \"{path}/item_pedido\" \"\"\").show()\n",
    "spark.sql(f\"\"\" CREATE TABLE IF NOT EXISTS silver.tbPagamentoPedido USING DELTA LOCATION  \"{path}/pagamento\" \"\"\").show()\n",
    "spark.sql(f\"\"\" CREATE TABLE IF NOT EXISTS silver.tbPedido USING DELTA LOCATION  \"{path}/pedido\" \"\"\").show()\n",
    "spark.sql(f\"\"\" CREATE TABLE IF NOT EXISTS silver.tbProduto USING DELTA LOCATION  \"{path}/produto\" \"\"\").show()\n",
    "spark.sql(f\"\"\" CREATE TABLE IF NOT EXISTS silver.tbVendedor USING DELTA LOCATION  \"{path}/vendedor\" \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------------+-----------+\n",
      "|namespace|        tableName|isTemporary|\n",
      "+---------+-----------------+-----------+\n",
      "|   silver|      tbavaliacao|      false|\n",
      "|   silver|        tbcliente|      false|\n",
      "|   silver| tbgeolocalizacao|      false|\n",
      "|   silver|     tbitempedido|      false|\n",
      "|   silver|tbpagamentopedido|      false|\n",
      "|   silver|         tbpedido|      false|\n",
      "|   silver|        tbproduto|      false|\n",
      "|   silver|       tbvendedor|      false|\n",
      "+---------+-----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\" SHOW TABLES \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/08 16:38:16 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 11:===========================================>            (39 + 4) / 50]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------+--------------------+----------------------+-----------+----------+\n",
      "|         idAvaliacao|            idPedido|vlNota|descTituloComentario|descMensagemComentario|dtAvaliacao|dtResposta|\n",
      "+--------------------+--------------------+------+--------------------+----------------------+-----------+----------+\n",
      "|a36150969fe6bd09e...|ef1b29b591d31d57c...|   1.0|                null|  Demora na entrega...| 2016-11-02|2016-11-03|\n",
      "|522f795c7ef411740...|25f680bf746bec3df...|   5.0|                null|  Produto conforme ...| 2016-11-02|2016-11-07|\n",
      "|dd065231d3137dfcb...|48c43415ea1c4af9a...|   3.0|                null|  gostei do produto...| 2016-11-02|2016-11-03|\n",
      "|58530d17ffb83e571...|f3f12fc90564a9b03...|   3.0|                null|  Entrega muito dem...| 2016-11-02|2016-11-05|\n",
      "|8b8ec505c06d139a8...|8729a8f7c27d03afe...|   5.0|                null|  Recebi o produto ...| 2016-11-02|2016-11-05|\n",
      "+--------------------+--------------------+------+--------------------+----------------------+-----------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.sql(\"SELECT * FROM silver.tbAvaliacao\")\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------+--------------------+------+\n",
      "|           idCliente|      idClienteUnico|codCep|          descCidade|descUF|\n",
      "+--------------------+--------------------+------+--------------------+------+\n",
      "|06b8999e2fba1a1fb...|861eff4711a542e4b...| 14409|              franca|    SP|\n",
      "|18955e83d337fd6b2...|290c77bc529b7ac93...|  9790|sao bernardo do c...|    SP|\n",
      "|4e7b3e00288586ebd...|060e732b5b29e8181...|  1151|           sao paulo|    SP|\n",
      "|b2b6027bc5c5109e5...|259dac757896d24d7...|  8775|     mogi das cruzes|    SP|\n",
      "|4f2d8ab171c80ec83...|345ecd01c38d18a90...| 13056|            campinas|    SP|\n",
      "+--------------------+--------------------+------+--------------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.sql(\"SELECT * FROM silver.tbcliente\")\n",
    "df.show(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
